{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJH1HZxT9rK6A/68hb9LgG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitryAlexandrovv/infopoisk/blob/master/task2/task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_ru')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmjHFjWaiJJu",
        "outputId": "a73dabd6-1fe8-41bf-b6f3-a3fa65c412de"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ru_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pBQp8e5ZKvF",
        "outputId": "dd31b2d6-c54b-4284-956d-ca885f2704f3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-03-08 13:25:25.761809: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-08 13:25:25.761961: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-08 13:25:25.761981: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-08 13:25:27.801041: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ru-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.4.0/ru_core_news_sm-3.4.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy2>=0.9\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from ru-core-news-sm==3.4.0) (3.4.4)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (23.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.22.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (8.1.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.4.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.25.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.10.5)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.26.14)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.1.2)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=d20a550f1c6b0b0a18262908a4ea229e6dccf3864e3205e216be05fb57ef417c\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2, ru-core-news-sm\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 ru-core-news-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy_langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZP6oYnVqTpD",
        "outputId": "31fed507-b6cb-4819-83d1-98a1a0fcd8e1"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy_langdetect\n",
            "  Downloading spacy_langdetect-0.1.2-py3-none-any.whl (5.0 kB)\n",
            "Collecting langdetect==1.0.7\n",
            "  Downloading langdetect-1.0.7.zip (998 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m998.1/998.1 KB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.8/dist-packages (from spacy_langdetect) (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from langdetect==1.0.7->spacy_langdetect) (1.15.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from pytest->spacy_langdetect) (1.11.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from pytest->spacy_langdetect) (22.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from pytest->spacy_langdetect) (57.4.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.8/dist-packages (from pytest->spacy_langdetect) (1.4.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest->spacy_langdetect) (9.1.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.8/dist-packages (from pytest->spacy_langdetect) (0.7.1)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.7-py3-none-any.whl size=993429 sha256=3a3b997813f7467170cd013606e978fc5144f7bacd244ac16974d221caf9fff0\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/6d/ab/bf9ecd1ab14dd236da586dfd0d4b008e2e803e571cf2229c26\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect, spacy_langdetect\n",
            "Successfully installed langdetect-1.0.7 spacy_langdetect-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKJ-z9ZPJu5l",
        "outputId": "b228fd60-8c77-4a78-ebbc-ff88da3b9666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "complete page 0\n",
            "complete page 1\n",
            "complete page 2\n",
            "complete page 3\n",
            "complete page 4\n",
            "complete page 5\n",
            "complete page 6\n",
            "complete page 7\n",
            "complete page 8\n",
            "complete page 9\n",
            "complete page 10\n",
            "complete page 11\n",
            "complete page 12\n",
            "complete page 13\n",
            "complete page 14\n",
            "complete page 15\n",
            "complete page 16\n",
            "complete page 17\n",
            "complete page 18\n",
            "complete page 19\n",
            "complete page 20\n",
            "complete page 21\n",
            "complete page 22\n",
            "complete page 23\n",
            "complete page 24\n",
            "complete page 25\n",
            "complete page 26\n",
            "complete page 27\n",
            "complete page 28\n",
            "complete page 29\n",
            "complete page 30\n",
            "complete page 31\n",
            "complete page 32\n",
            "complete page 33\n",
            "complete page 34\n",
            "complete page 35\n",
            "complete page 36\n",
            "complete page 37\n",
            "complete page 38\n",
            "complete page 39\n",
            "complete page 40\n",
            "complete page 41\n",
            "complete page 42\n",
            "complete page 43\n",
            "complete page 44\n",
            "complete page 45\n",
            "complete page 46\n",
            "complete page 47\n",
            "complete page 48\n",
            "complete page 49\n",
            "complete page 50\n",
            "complete page 51\n",
            "complete page 52\n",
            "complete page 53\n",
            "complete page 54\n",
            "complete page 55\n",
            "complete page 56\n",
            "complete page 57\n",
            "complete page 58\n",
            "complete page 59\n",
            "complete page 60\n",
            "complete page 61\n",
            "complete page 62\n",
            "complete page 63\n",
            "complete page 64\n",
            "complete page 65\n",
            "complete page 66\n",
            "complete page 67\n",
            "complete page 68\n",
            "complete page 69\n",
            "complete page 70\n",
            "complete page 71\n",
            "complete page 72\n",
            "complete page 73\n",
            "complete page 74\n",
            "complete page 75\n",
            "complete page 76\n",
            "complete page 77\n",
            "complete page 78\n",
            "complete page 79\n",
            "complete page 80\n",
            "complete page 81\n",
            "complete page 82\n",
            "complete page 83\n",
            "complete page 84\n",
            "complete page 85\n",
            "complete page 86\n",
            "complete page 87\n",
            "complete page 88\n",
            "complete page 89\n",
            "complete page 90\n",
            "complete page 91\n",
            "complete page 92\n",
            "complete page 93\n",
            "complete page 94\n",
            "complete page 95\n",
            "complete page 96\n",
            "complete page 97\n",
            "complete page 98\n",
            "complete page 99\n"
          ]
        }
      ],
      "source": [
        "from html.parser import HTMLParser\n",
        "from bs4 import BeautifulSoup\n",
        "from gensim.utils import tokenize\n",
        "import spacy\n",
        "import nltk\n",
        "import string\n",
        "import pymorphy2\n",
        "from spacy.language import Language\n",
        "from spacy_langdetect import LanguageDetector\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def get_text_from_html(html):\n",
        "    soup = BeautifulSoup(data, features=\"html.parser\")\n",
        "\n",
        "    for script in soup([\"script\", \"style\"]):\n",
        "        script.extract()    # rip it out\n",
        "\n",
        "    text = soup.get_text(' ')\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "    return text\n",
        "\n",
        "def pos(word, morth=pymorphy2.MorphAnalyzer()):\n",
        "    return morth.parse(word)[0].tag.POS\n",
        "\n",
        "def get_lang_detector(nlp, name):\n",
        "    return LanguageDetector()\n",
        "\n",
        "load_model = spacy.load('ru_core_news_sm')\n",
        "dictionary = {}\n",
        "tokens = []\n",
        "\n",
        "for i in range(100):\n",
        "    with open('drive/MyDrive/dz/pages/page' + str(i) + '.html', 'r') as file:\n",
        "        data = file.read()\n",
        "        text = get_text_from_html(data)\n",
        "\n",
        "        # Убираем предлоги и союзы, знаки препинания\n",
        "        words = text.split()\n",
        "        functors_pos = {'CONJ', 'PREP'}  # function words\n",
        "        tt = str.maketrans(dict.fromkeys(string.punctuation))\n",
        "        prepared_text = ' '.join([word for word in words if pos(word) not in functors_pos]).translate(tt)\n",
        "\n",
        "        doc = load_model(prepared_text.lower())\n",
        "        for token in doc:\n",
        "            if token.is_digit or len(str(token).strip()) < 1 or '\"' in str(token) or token._.language['language'] != 'ru':\n",
        "                continue\n",
        "            if dictionary.get(token.lemma_) and dictionary.get(token.lemma_).find(str(token)) == -1:\n",
        "                dictionary.update({token.lemma_: dictionary.get(token.lemma_) + ' ' + str(token)})\n",
        "            else:\n",
        "                dictionary.update({token.lemma_: str(token)})\n",
        "            \n",
        "            if (not str(token) in tokens):\n",
        "                tokens.append(str(token))\n",
        "\n",
        "    print('complete page ' + str(i))\n",
        "\n",
        "with open('drive/MyDrive/dz/tokens.txt', 'w') as writefile:\n",
        "    for token in tokens:\n",
        "        writefile.write(token + '\\n')\n",
        "\n",
        "with open('drive/MyDrive/dz/lemmas.txt', 'w') as writefile:\n",
        "    for value in dictionary:\n",
        "        writefile.write(value + ': ' + dictionary.get(value) + '\\n')\n",
        "\n"
      ]
    }
  ]
}